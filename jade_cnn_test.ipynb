{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jade_cnn_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMucRDlSoLLtewlN21fuu9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JadeKim/Quanvolution-lab/blob/main/jade_cnn_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvwcnqL6P02g"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn # basic neural network\n",
        "import torch.nn.functional as F # Convolutional functions\n",
        "import torch.optim as optim # optimizers\n",
        "from torchvision import datasets, transforms # image datasets, image transformations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmYfRTNETTjN",
        "outputId": "25a6fedc-7445-4c2e-eacf-1394c4d3c46e"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grfAnq7tQFbR"
      },
      "source": [
        "trn_dataset = datasets.MNIST('../mnist_data/',\n",
        "                             download=True,\n",
        "                             train=True,\n",
        "                             transform=transforms.Compose([\n",
        "                                 transforms.ToTensor(), # image to Tensor\n",
        "                                 transforms.Normalize((0.1307,), (0.3081,)) # mean, std of MNIST.\n",
        "                             ])) \n",
        "\n",
        "val_dataset = datasets.MNIST(\"../mnist_data/\", \n",
        "                             download=False,\n",
        "                             train=False,\n",
        "                             transform= transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307, ),(0.3081, ))\n",
        "                           ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dmEHvAKX5_1",
        "outputId": "2a90ff75-020d-4405-c03b-189b011b5a81"
      },
      "source": [
        "print(type(trn_dataset))\n",
        "print(len(trn_dataset))\n",
        "print(len(val_dataset))\n",
        "print(len(trn_dataset[0][0][0][0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torchvision.datasets.mnist.MNIST'>\n",
            "60000\n",
            "10000\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmP_q_VxQJxK"
      },
      "source": [
        "# Batch iterator\n",
        "batch_size = 6\n",
        "trn_loader = torch.utils.data.DataLoader(trn_dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD0wnNkS7sc_",
        "outputId": "6a368bf3-e578-432b-f9d6-17b08e147f35"
      },
      "source": [
        "input = torch.Tensor(1, 1, 28, 28)\n",
        "conv1 = nn.Conv2d(in_channels=1, out_channels=50, kernel_size=5) # 50@24*24\n",
        "# activation ReLU\n",
        "pool1 = nn.MaxPool2d(2) # 50@12*12\n",
        "conv2 = nn.Conv2d(in_channels=50, out_channels=64, kernel_size=5) # 64@8*8\n",
        "# activation ReLU\n",
        "pool2 = nn.MaxPool2d(2) # 64@4*4\n",
        "output = pool2(conv2(pool1(conv1(input))))\n",
        "fc1 = output.view(output.size(0), -1)\n",
        "do = nn.Dropout(p=0.4)\n",
        "fc2 = nn.Linear(1024, 10)\n",
        "print(conv1(input).shape)\n",
        "print(pool1(conv1(input)).shape)\n",
        "print(conv2(pool1(conv1(input))).shape)\n",
        "print(output.shape)\n",
        "print(fc1.shape)\n",
        "print(do(fc1).shape)\n",
        "print(fc2(do(fc1)).shape)\n",
        "\n",
        "# Result:\n",
        "# torch.Size([1, 50, 24, 24])\n",
        "# torch.Size([1, 50, 12, 12])\n",
        "# torch.Size([1, 64, 8, 8])\n",
        "# torch.Size([1, 64, 4, 4])\n",
        "# torch.Size([1, 1024])\n",
        "# torch.Size([1, 1024])\n",
        "# torch.Size([1, 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 50, 24, 24])\n",
            "torch.Size([1, 50, 12, 12])\n",
            "torch.Size([1, 64, 8, 8])\n",
            "torch.Size([1, 64, 4, 4])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV5Nea87RUfs"
      },
      "source": [
        "# construct model on cuda if available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, model='cnn'):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        \n",
        "        if model == 'random':\n",
        "            self.conv_module = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=1, out_channels=50, kernel_size=5), # 50@24*24\n",
        "                nn.Sigmoid(),\n",
        "                nn.MaxPool2d(2), # 50@12*12\n",
        "                nn.Conv2d(in_channels=50, out_channels=64, kernel_size=5), # 64@8*8\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2) # 64@4*4\n",
        "            )\n",
        "        else:\n",
        "            self.conv_module = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=1, out_channels=50, kernel_size=5), # 50@24*24\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2), # 50@12*12\n",
        "                nn.Conv2d(in_channels=50, out_channels=64, kernel_size=5), # 64@8*8\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2) # 64@4*4\n",
        "            )\n",
        "\n",
        "        self.fc_module = nn.Sequential(\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "        \n",
        "        # cuda\n",
        "        if use_cuda:\n",
        "            self.conv_module = self.conv_module.cuda()\n",
        "            self.fc_module = self.fc_module.cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv_module(x) \n",
        "        # 64@4*4\n",
        "        out = out.view(out.size(0), -1) # 1*1024\n",
        "        out = self.fc_module(out)\n",
        "        return F.softmax(out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxgBSM4dR69_",
        "outputId": "7dbc9637-094a-4d4c-8085-0525c8264189"
      },
      "source": [
        "cnn = CNNClassifier()\n",
        "random = CNNClassifier('random')\n",
        "print(cnn)\n",
        "print(random)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNNClassifier(\n",
            "  (conv_module): Sequential(\n",
            "    (0): Conv2d(1, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(50, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_module): Sequential(\n",
            "    (0): Dropout(p=0.4, inplace=False)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "CNNClassifier(\n",
            "  (conv_module): Sequential(\n",
            "    (0): Conv2d(1, 50, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Sigmoid()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(50, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_module): Sequential(\n",
            "    (0): Dropout(p=0.4, inplace=False)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWnYxg4kRfta",
        "outputId": "5a7f008a-cb6b-4731-9ffc-94f59c4118fe"
      },
      "source": [
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# backpropagation method\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
        "optimizer_r = optim.Adam(random.parameters(), lr=learning_rate)\n",
        "# hyper-parameters\n",
        "num_epochs = 1\n",
        "num_batches = len(trn_loader)\n",
        "\n",
        "trn_loss_list = []\n",
        "val_loss_list = []\n",
        "acc_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    trn_loss = 0.0\n",
        "    correct = 0.0\n",
        "    for i, data in enumerate(trn_loader):\n",
        "        x, label = data\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "            label = label.cuda()\n",
        "        # grad init\n",
        "        optimizer.zero_grad()\n",
        "        # forward propagation\n",
        "        model_output = cnn(x)\n",
        "        _, predicted = torch.max(model_output.data, 1)\n",
        "        # calculate loss\n",
        "        loss = criterion(model_output, label)\n",
        "        # back propagation \n",
        "        loss.backward()\n",
        "        # weight update\n",
        "        optimizer.step()\n",
        "        \n",
        "        # trn_loss summary\n",
        "        trn_loss += loss.item()\n",
        "        correct += (predicted == label).float().sum()/batch_size\n",
        "\n",
        "        # del (memory issue)\n",
        "        del loss\n",
        "        del model_output\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0.0\n",
        "                for j, val in enumerate(val_loader):\n",
        "                    val_x, val_label = val\n",
        "                    if use_cuda:\n",
        "                        val_x = val_x.cuda()\n",
        "                        val_label =val_label.cuda()\n",
        "                    val_output = cnn(val_x)\n",
        "                    v_loss = criterion(val_output, val_label)\n",
        "                    val_loss += v_loss\n",
        "                       \n",
        "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | trn acc: {:.4f}\".format(\n",
        "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader), correct / (i+1)\n",
        "            ))            \n",
        "            \n",
        "            trn_loss_list.append(trn_loss/100)\n",
        "            val_loss_list.append(val_loss/len(val_loader))\n",
        "            acc_list.append(correct / (i+1))\n",
        "            trn_loss = 0.0\n",
        "print()\n",
        "trn_loss_list_r = []\n",
        "val_loss_list_r = []\n",
        "acc_list_r = []\n",
        "for epoch in range(num_epochs):\n",
        "    trn_loss = 0.0\n",
        "    correct = 0.0\n",
        "    for i, data in enumerate(trn_loader):\n",
        "        x, label = data\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "            label = label.cuda()\n",
        "        # grad init\n",
        "        optimizer_r.zero_grad()\n",
        "        # forward propagation\n",
        "        model_output = random(x)\n",
        "        _, predicted = torch.max(model_output.data, 1)\n",
        "        # calculate loss\n",
        "        loss = criterion(model_output, label)\n",
        "        # back propagation \n",
        "        loss.backward()\n",
        "        # weight update\n",
        "        optimizer_r.step()\n",
        "        \n",
        "        # trn_loss summary\n",
        "        trn_loss += loss.item()\n",
        "        correct += (predicted == label).float().sum()/batch_size\n",
        "\n",
        "        # del (memory issue)\n",
        "        del loss\n",
        "        del model_output\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            with torch.no_grad():\n",
        "                val_loss = 0.0\n",
        "                for j, val in enumerate(val_loader):\n",
        "                    val_x, val_label = val\n",
        "                    if use_cuda:\n",
        "                        val_x = val_x.cuda()\n",
        "                        val_label =val_label.cuda()\n",
        "                    val_output = random(val_x)\n",
        "                    v_loss = criterion(val_output, val_label)\n",
        "                    val_loss += v_loss\n",
        "                       \n",
        "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | trn acc: {:.4f}\".format(\n",
        "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader), correct / (i+1)\n",
        "            ))            \n",
        "            \n",
        "            trn_loss_list_r.append(trn_loss/100)\n",
        "            val_loss_list_r.append(val_loss/len(val_loader))\n",
        "            acc_list_r.append(correct / (i+1))\n",
        "            trn_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1/1 | step: 100/10000 | trn loss: 2.0933 | val loss: 1.8657 | trn acc: 0.3733\n",
            "epoch: 1/1 | step: 200/10000 | trn loss: 1.7949 | val loss: 1.7931 | trn acc: 0.5200\n",
            "epoch: 1/1 | step: 300/10000 | trn loss: 1.7229 | val loss: 1.6664 | trn acc: 0.5939\n",
            "epoch: 1/1 | step: 400/10000 | trn loss: 1.6731 | val loss: 1.6347 | trn acc: 0.6429\n",
            "epoch: 1/1 | step: 500/10000 | trn loss: 1.6441 | val loss: 1.6305 | trn acc: 0.6773\n",
            "epoch: 1/1 | step: 600/10000 | trn loss: 1.6148 | val loss: 1.6225 | trn acc: 0.7067\n",
            "epoch: 1/1 | step: 700/10000 | trn loss: 1.6380 | val loss: 1.6197 | trn acc: 0.7236\n",
            "epoch: 1/1 | step: 800/10000 | trn loss: 1.6050 | val loss: 1.6324 | trn acc: 0.7400\n",
            "epoch: 1/1 | step: 900/10000 | trn loss: 1.6257 | val loss: 1.5765 | trn acc: 0.7504\n",
            "epoch: 1/1 | step: 1000/10000 | trn loss: 1.5472 | val loss: 1.5420 | trn acc: 0.7668\n",
            "epoch: 1/1 | step: 1100/10000 | trn loss: 1.5628 | val loss: 1.5265 | trn acc: 0.7792\n",
            "epoch: 1/1 | step: 1200/10000 | trn loss: 1.5431 | val loss: 1.5364 | trn acc: 0.7914\n",
            "epoch: 1/1 | step: 1300/10000 | trn loss: 1.5461 | val loss: 1.5387 | trn acc: 0.8014\n",
            "epoch: 1/1 | step: 1400/10000 | trn loss: 1.5544 | val loss: 1.5275 | trn acc: 0.8095\n",
            "epoch: 1/1 | step: 1500/10000 | trn loss: 1.5455 | val loss: 1.5295 | trn acc: 0.8170\n",
            "epoch: 1/1 | step: 1600/10000 | trn loss: 1.5386 | val loss: 1.5230 | trn acc: 0.8240\n",
            "epoch: 1/1 | step: 1700/10000 | trn loss: 1.5217 | val loss: 1.5216 | trn acc: 0.8308\n",
            "epoch: 1/1 | step: 1800/10000 | trn loss: 1.5290 | val loss: 1.5202 | trn acc: 0.8364\n",
            "epoch: 1/1 | step: 1900/10000 | trn loss: 1.5426 | val loss: 1.5267 | trn acc: 0.8407\n",
            "epoch: 1/1 | step: 2000/10000 | trn loss: 1.5353 | val loss: 1.5263 | trn acc: 0.8452\n",
            "epoch: 1/1 | step: 2100/10000 | trn loss: 1.5405 | val loss: 1.5171 | trn acc: 0.8487\n",
            "epoch: 1/1 | step: 2200/10000 | trn loss: 1.5121 | val loss: 1.5107 | trn acc: 0.8534\n",
            "epoch: 1/1 | step: 2300/10000 | trn loss: 1.5151 | val loss: 1.5205 | trn acc: 0.8575\n",
            "epoch: 1/1 | step: 2400/10000 | trn loss: 1.5175 | val loss: 1.5123 | trn acc: 0.8613\n",
            "epoch: 1/1 | step: 2500/10000 | trn loss: 1.5049 | val loss: 1.5071 | trn acc: 0.8651\n",
            "epoch: 1/1 | step: 2600/10000 | trn loss: 1.5321 | val loss: 1.5163 | trn acc: 0.8676\n",
            "epoch: 1/1 | step: 2700/10000 | trn loss: 1.5074 | val loss: 1.5113 | trn acc: 0.8709\n",
            "epoch: 1/1 | step: 2800/10000 | trn loss: 1.5044 | val loss: 1.5074 | trn acc: 0.8740\n",
            "epoch: 1/1 | step: 2900/10000 | trn loss: 1.5087 | val loss: 1.5058 | trn acc: 0.8768\n",
            "epoch: 1/1 | step: 3000/10000 | trn loss: 1.5100 | val loss: 1.5038 | trn acc: 0.8792\n",
            "epoch: 1/1 | step: 3100/10000 | trn loss: 1.5084 | val loss: 1.5038 | trn acc: 0.8817\n",
            "epoch: 1/1 | step: 3200/10000 | trn loss: 1.5145 | val loss: 1.5060 | trn acc: 0.8836\n",
            "epoch: 1/1 | step: 3300/10000 | trn loss: 1.5146 | val loss: 1.5125 | trn acc: 0.8855\n",
            "epoch: 1/1 | step: 3400/10000 | trn loss: 1.5141 | val loss: 1.5051 | trn acc: 0.8873\n",
            "epoch: 1/1 | step: 3500/10000 | trn loss: 1.5279 | val loss: 1.5020 | trn acc: 0.8887\n",
            "epoch: 1/1 | step: 3600/10000 | trn loss: 1.5097 | val loss: 1.5040 | trn acc: 0.8904\n",
            "epoch: 1/1 | step: 3700/10000 | trn loss: 1.5110 | val loss: 1.5164 | trn acc: 0.8919\n",
            "epoch: 1/1 | step: 3800/10000 | trn loss: 1.5091 | val loss: 1.5116 | trn acc: 0.8936\n",
            "epoch: 1/1 | step: 3900/10000 | trn loss: 1.5069 | val loss: 1.5136 | trn acc: 0.8952\n",
            "epoch: 1/1 | step: 4000/10000 | trn loss: 1.5075 | val loss: 1.5082 | trn acc: 0.8968\n",
            "epoch: 1/1 | step: 4100/10000 | trn loss: 1.5067 | val loss: 1.5140 | trn acc: 0.8983\n",
            "epoch: 1/1 | step: 4200/10000 | trn loss: 1.5066 | val loss: 1.5107 | trn acc: 0.8996\n",
            "epoch: 1/1 | step: 4300/10000 | trn loss: 1.5153 | val loss: 1.5024 | trn acc: 0.9007\n",
            "epoch: 1/1 | step: 4400/10000 | trn loss: 1.4991 | val loss: 1.5051 | trn acc: 0.9021\n",
            "epoch: 1/1 | step: 4500/10000 | trn loss: 1.4975 | val loss: 1.5008 | trn acc: 0.9035\n",
            "epoch: 1/1 | step: 4600/10000 | trn loss: 1.5050 | val loss: 1.5001 | trn acc: 0.9047\n",
            "epoch: 1/1 | step: 4700/10000 | trn loss: 1.4983 | val loss: 1.5092 | trn acc: 0.9061\n",
            "epoch: 1/1 | step: 4800/10000 | trn loss: 1.5125 | val loss: 1.5032 | trn acc: 0.9069\n",
            "epoch: 1/1 | step: 4900/10000 | trn loss: 1.5102 | val loss: 1.5009 | trn acc: 0.9078\n",
            "epoch: 1/1 | step: 5000/10000 | trn loss: 1.5250 | val loss: 1.4979 | trn acc: 0.9084\n",
            "epoch: 1/1 | step: 5100/10000 | trn loss: 1.4987 | val loss: 1.4967 | trn acc: 0.9094\n",
            "epoch: 1/1 | step: 5200/10000 | trn loss: 1.5164 | val loss: 1.4966 | trn acc: 0.9101\n",
            "epoch: 1/1 | step: 5300/10000 | trn loss: 1.5027 | val loss: 1.4950 | trn acc: 0.9110\n",
            "epoch: 1/1 | step: 5400/10000 | trn loss: 1.5281 | val loss: 1.5023 | trn acc: 0.9115\n",
            "epoch: 1/1 | step: 5500/10000 | trn loss: 1.5089 | val loss: 1.5076 | trn acc: 0.9122\n",
            "epoch: 1/1 | step: 5600/10000 | trn loss: 1.4999 | val loss: 1.5053 | trn acc: 0.9132\n",
            "epoch: 1/1 | step: 5700/10000 | trn loss: 1.5025 | val loss: 1.5002 | trn acc: 0.9140\n",
            "epoch: 1/1 | step: 5800/10000 | trn loss: 1.4974 | val loss: 1.5035 | trn acc: 0.9148\n",
            "epoch: 1/1 | step: 5900/10000 | trn loss: 1.5120 | val loss: 1.5048 | trn acc: 0.9154\n",
            "epoch: 1/1 | step: 6000/10000 | trn loss: 1.5010 | val loss: 1.5050 | trn acc: 0.9161\n",
            "epoch: 1/1 | step: 6100/10000 | trn loss: 1.4989 | val loss: 1.4956 | trn acc: 0.9169\n",
            "epoch: 1/1 | step: 6200/10000 | trn loss: 1.5089 | val loss: 1.4983 | trn acc: 0.9175\n",
            "epoch: 1/1 | step: 6300/10000 | trn loss: 1.5043 | val loss: 1.4983 | trn acc: 0.9181\n",
            "epoch: 1/1 | step: 6400/10000 | trn loss: 1.5001 | val loss: 1.4983 | trn acc: 0.9188\n",
            "epoch: 1/1 | step: 6500/10000 | trn loss: 1.4956 | val loss: 1.5130 | trn acc: 0.9195\n",
            "epoch: 1/1 | step: 6600/10000 | trn loss: 1.5071 | val loss: 1.4987 | trn acc: 0.9200\n",
            "epoch: 1/1 | step: 6700/10000 | trn loss: 1.4894 | val loss: 1.4956 | trn acc: 0.9208\n",
            "epoch: 1/1 | step: 6800/10000 | trn loss: 1.4958 | val loss: 1.5014 | trn acc: 0.9215\n",
            "epoch: 1/1 | step: 6900/10000 | trn loss: 1.5111 | val loss: 1.4962 | trn acc: 0.9218\n",
            "epoch: 1/1 | step: 7000/10000 | trn loss: 1.5112 | val loss: 1.4975 | trn acc: 0.9222\n",
            "epoch: 1/1 | step: 7100/10000 | trn loss: 1.5084 | val loss: 1.5062 | trn acc: 0.9227\n",
            "epoch: 1/1 | step: 7200/10000 | trn loss: 1.5207 | val loss: 1.4993 | trn acc: 0.9229\n",
            "epoch: 1/1 | step: 7300/10000 | trn loss: 1.5212 | val loss: 1.4950 | trn acc: 0.9231\n",
            "epoch: 1/1 | step: 7400/10000 | trn loss: 1.5074 | val loss: 1.4959 | trn acc: 0.9236\n",
            "epoch: 1/1 | step: 7500/10000 | trn loss: 1.5091 | val loss: 1.5087 | trn acc: 0.9240\n",
            "epoch: 1/1 | step: 7600/10000 | trn loss: 1.5069 | val loss: 1.4951 | trn acc: 0.9244\n",
            "epoch: 1/1 | step: 7700/10000 | trn loss: 1.5126 | val loss: 1.5048 | trn acc: 0.9247\n",
            "epoch: 1/1 | step: 7800/10000 | trn loss: 1.4911 | val loss: 1.4953 | trn acc: 0.9253\n",
            "epoch: 1/1 | step: 7900/10000 | trn loss: 1.4979 | val loss: 1.4951 | trn acc: 0.9258\n",
            "epoch: 1/1 | step: 8000/10000 | trn loss: 1.4955 | val loss: 1.4987 | trn acc: 0.9263\n",
            "epoch: 1/1 | step: 8100/10000 | trn loss: 1.5019 | val loss: 1.4979 | trn acc: 0.9267\n",
            "epoch: 1/1 | step: 8200/10000 | trn loss: 1.5091 | val loss: 1.5016 | trn acc: 0.9271\n",
            "epoch: 1/1 | step: 8300/10000 | trn loss: 1.4824 | val loss: 1.4959 | trn acc: 0.9277\n",
            "epoch: 1/1 | step: 8400/10000 | trn loss: 1.4976 | val loss: 1.5129 | trn acc: 0.9281\n",
            "epoch: 1/1 | step: 8500/10000 | trn loss: 1.5023 | val loss: 1.4945 | trn acc: 0.9284\n",
            "epoch: 1/1 | step: 8600/10000 | trn loss: 1.4937 | val loss: 1.5043 | trn acc: 0.9289\n",
            "epoch: 1/1 | step: 8700/10000 | trn loss: 1.5313 | val loss: 1.5056 | trn acc: 0.9289\n",
            "epoch: 1/1 | step: 8800/10000 | trn loss: 1.5040 | val loss: 1.5190 | trn acc: 0.9292\n",
            "epoch: 1/1 | step: 8900/10000 | trn loss: 1.5145 | val loss: 1.5005 | trn acc: 0.9294\n",
            "epoch: 1/1 | step: 9000/10000 | trn loss: 1.4950 | val loss: 1.4987 | trn acc: 0.9298\n",
            "epoch: 1/1 | step: 9100/10000 | trn loss: 1.5003 | val loss: 1.5007 | trn acc: 0.9301\n",
            "epoch: 1/1 | step: 9200/10000 | trn loss: 1.5031 | val loss: 1.4945 | trn acc: 0.9304\n",
            "epoch: 1/1 | step: 9300/10000 | trn loss: 1.4984 | val loss: 1.4904 | trn acc: 0.9308\n",
            "epoch: 1/1 | step: 9400/10000 | trn loss: 1.5013 | val loss: 1.5003 | trn acc: 0.9310\n",
            "epoch: 1/1 | step: 9500/10000 | trn loss: 1.5128 | val loss: 1.4975 | trn acc: 0.9312\n",
            "epoch: 1/1 | step: 9600/10000 | trn loss: 1.5073 | val loss: 1.4902 | trn acc: 0.9315\n",
            "epoch: 1/1 | step: 9700/10000 | trn loss: 1.5039 | val loss: 1.5002 | trn acc: 0.9317\n",
            "epoch: 1/1 | step: 9800/10000 | trn loss: 1.5113 | val loss: 1.4933 | trn acc: 0.9319\n",
            "epoch: 1/1 | step: 9900/10000 | trn loss: 1.5072 | val loss: 1.4895 | trn acc: 0.9321\n",
            "epoch: 1/1 | step: 10000/10000 | trn loss: 1.4938 | val loss: 1.5132 | trn acc: 0.9325\n",
            "\n",
            "epoch: 1/1 | step: 100/10000 | trn loss: 2.3075 | val loss: 2.3005 | trn acc: 0.1067\n",
            "epoch: 1/1 | step: 200/10000 | trn loss: 2.3027 | val loss: 2.3007 | trn acc: 0.1117\n",
            "epoch: 1/1 | step: 300/10000 | trn loss: 2.2998 | val loss: 2.2993 | trn acc: 0.1222\n",
            "epoch: 1/1 | step: 400/10000 | trn loss: 2.2806 | val loss: 2.2487 | trn acc: 0.1313\n",
            "epoch: 1/1 | step: 500/10000 | trn loss: 2.2129 | val loss: 2.1556 | trn acc: 0.1517\n",
            "epoch: 1/1 | step: 600/10000 | trn loss: 2.1420 | val loss: 2.0452 | trn acc: 0.1814\n",
            "epoch: 1/1 | step: 700/10000 | trn loss: 2.0475 | val loss: 1.9799 | trn acc: 0.2167\n",
            "epoch: 1/1 | step: 800/10000 | trn loss: 1.9734 | val loss: 1.9483 | trn acc: 0.2527\n",
            "epoch: 1/1 | step: 900/10000 | trn loss: 1.9017 | val loss: 1.9013 | trn acc: 0.2885\n",
            "epoch: 1/1 | step: 1000/10000 | trn loss: 1.9155 | val loss: 1.8778 | trn acc: 0.3145\n",
            "epoch: 1/1 | step: 1100/10000 | trn loss: 1.8754 | val loss: 1.8822 | trn acc: 0.3406\n",
            "epoch: 1/1 | step: 1200/10000 | trn loss: 1.8804 | val loss: 1.8454 | trn acc: 0.3619\n",
            "epoch: 1/1 | step: 1300/10000 | trn loss: 1.8493 | val loss: 1.8372 | trn acc: 0.3823\n",
            "epoch: 1/1 | step: 1400/10000 | trn loss: 1.8601 | val loss: 1.8317 | trn acc: 0.3990\n",
            "epoch: 1/1 | step: 1500/10000 | trn loss: 1.8268 | val loss: 1.8167 | trn acc: 0.4147\n",
            "epoch: 1/1 | step: 1600/10000 | trn loss: 1.7925 | val loss: 1.8127 | trn acc: 0.4303\n",
            "epoch: 1/1 | step: 1700/10000 | trn loss: 1.8071 | val loss: 1.8085 | trn acc: 0.4442\n",
            "epoch: 1/1 | step: 1800/10000 | trn loss: 1.8055 | val loss: 1.7963 | trn acc: 0.4563\n",
            "epoch: 1/1 | step: 1900/10000 | trn loss: 1.7964 | val loss: 1.7900 | trn acc: 0.4683\n",
            "epoch: 1/1 | step: 2000/10000 | trn loss: 1.7968 | val loss: 1.7734 | trn acc: 0.4790\n",
            "epoch: 1/1 | step: 2100/10000 | trn loss: 1.7971 | val loss: 1.7815 | trn acc: 0.4883\n",
            "epoch: 1/1 | step: 2200/10000 | trn loss: 1.7944 | val loss: 1.7676 | trn acc: 0.4970\n",
            "epoch: 1/1 | step: 2300/10000 | trn loss: 1.7878 | val loss: 1.7788 | trn acc: 0.5053\n",
            "epoch: 1/1 | step: 2400/10000 | trn loss: 1.7704 | val loss: 1.7695 | trn acc: 0.5134\n",
            "epoch: 1/1 | step: 2500/10000 | trn loss: 1.7754 | val loss: 1.7701 | trn acc: 0.5207\n",
            "epoch: 1/1 | step: 2600/10000 | trn loss: 1.7763 | val loss: 1.7619 | trn acc: 0.5272\n",
            "epoch: 1/1 | step: 2700/10000 | trn loss: 1.7675 | val loss: 1.7811 | trn acc: 0.5339\n",
            "epoch: 1/1 | step: 2800/10000 | trn loss: 1.7817 | val loss: 1.7669 | trn acc: 0.5394\n",
            "epoch: 1/1 | step: 2900/10000 | trn loss: 1.7901 | val loss: 1.7540 | trn acc: 0.5439\n",
            "epoch: 1/1 | step: 3000/10000 | trn loss: 1.7891 | val loss: 1.7636 | trn acc: 0.5484\n",
            "epoch: 1/1 | step: 3100/10000 | trn loss: 1.7600 | val loss: 1.7547 | trn acc: 0.5535\n",
            "epoch: 1/1 | step: 3200/10000 | trn loss: 1.7649 | val loss: 1.7535 | trn acc: 0.5577\n",
            "epoch: 1/1 | step: 3300/10000 | trn loss: 1.7649 | val loss: 1.7478 | trn acc: 0.5621\n",
            "epoch: 1/1 | step: 3400/10000 | trn loss: 1.7382 | val loss: 1.7506 | trn acc: 0.5667\n",
            "epoch: 1/1 | step: 3500/10000 | trn loss: 1.7597 | val loss: 1.7387 | trn acc: 0.5710\n",
            "epoch: 1/1 | step: 3600/10000 | trn loss: 1.7655 | val loss: 1.7435 | trn acc: 0.5745\n",
            "epoch: 1/1 | step: 3700/10000 | trn loss: 1.7729 | val loss: 1.7440 | trn acc: 0.5776\n",
            "epoch: 1/1 | step: 3800/10000 | trn loss: 1.7455 | val loss: 1.7492 | trn acc: 0.5814\n",
            "epoch: 1/1 | step: 3900/10000 | trn loss: 1.7773 | val loss: 1.7396 | trn acc: 0.5843\n",
            "epoch: 1/1 | step: 4000/10000 | trn loss: 1.7424 | val loss: 1.7427 | trn acc: 0.5880\n",
            "epoch: 1/1 | step: 4100/10000 | trn loss: 1.7440 | val loss: 1.7478 | trn acc: 0.5913\n",
            "epoch: 1/1 | step: 4200/10000 | trn loss: 1.7520 | val loss: 1.7421 | trn acc: 0.5944\n",
            "epoch: 1/1 | step: 4300/10000 | trn loss: 1.7544 | val loss: 1.7425 | trn acc: 0.5969\n",
            "epoch: 1/1 | step: 4400/10000 | trn loss: 1.7575 | val loss: 1.7385 | trn acc: 0.5996\n",
            "epoch: 1/1 | step: 4500/10000 | trn loss: 1.7799 | val loss: 1.7372 | trn acc: 0.6014\n",
            "epoch: 1/1 | step: 4600/10000 | trn loss: 1.7054 | val loss: 1.7430 | trn acc: 0.6050\n",
            "epoch: 1/1 | step: 4700/10000 | trn loss: 1.7426 | val loss: 1.7392 | trn acc: 0.6073\n",
            "epoch: 1/1 | step: 4800/10000 | trn loss: 1.7292 | val loss: 1.7265 | trn acc: 0.6100\n",
            "epoch: 1/1 | step: 4900/10000 | trn loss: 1.7335 | val loss: 1.7418 | trn acc: 0.6125\n",
            "epoch: 1/1 | step: 5000/10000 | trn loss: 1.7421 | val loss: 1.7482 | trn acc: 0.6146\n",
            "epoch: 1/1 | step: 5100/10000 | trn loss: 1.7263 | val loss: 1.7332 | trn acc: 0.6170\n",
            "epoch: 1/1 | step: 5200/10000 | trn loss: 1.7179 | val loss: 1.7328 | trn acc: 0.6195\n",
            "epoch: 1/1 | step: 5300/10000 | trn loss: 1.7541 | val loss: 1.7282 | trn acc: 0.6212\n",
            "epoch: 1/1 | step: 5400/10000 | trn loss: 1.7189 | val loss: 1.7296 | trn acc: 0.6235\n",
            "epoch: 1/1 | step: 5500/10000 | trn loss: 1.7393 | val loss: 1.7287 | trn acc: 0.6253\n",
            "epoch: 1/1 | step: 5600/10000 | trn loss: 1.7304 | val loss: 1.7231 | trn acc: 0.6273\n",
            "epoch: 1/1 | step: 5700/10000 | trn loss: 1.7522 | val loss: 1.7253 | trn acc: 0.6287\n",
            "epoch: 1/1 | step: 5800/10000 | trn loss: 1.7193 | val loss: 1.7148 | trn acc: 0.6309\n",
            "epoch: 1/1 | step: 5900/10000 | trn loss: 1.7175 | val loss: 1.7217 | trn acc: 0.6329\n",
            "epoch: 1/1 | step: 6000/10000 | trn loss: 1.7142 | val loss: 1.7205 | trn acc: 0.6350\n",
            "epoch: 1/1 | step: 6100/10000 | trn loss: 1.7446 | val loss: 1.7239 | trn acc: 0.6363\n",
            "epoch: 1/1 | step: 6200/10000 | trn loss: 1.7390 | val loss: 1.7148 | trn acc: 0.6378\n",
            "epoch: 1/1 | step: 6300/10000 | trn loss: 1.7311 | val loss: 1.7202 | trn acc: 0.6392\n",
            "epoch: 1/1 | step: 6400/10000 | trn loss: 1.7344 | val loss: 1.7159 | trn acc: 0.6406\n",
            "epoch: 1/1 | step: 6500/10000 | trn loss: 1.7126 | val loss: 1.7224 | trn acc: 0.6424\n",
            "epoch: 1/1 | step: 6600/10000 | trn loss: 1.7247 | val loss: 1.7127 | trn acc: 0.6438\n",
            "epoch: 1/1 | step: 6700/10000 | trn loss: 1.7510 | val loss: 1.7358 | trn acc: 0.6448\n",
            "epoch: 1/1 | step: 6800/10000 | trn loss: 1.7282 | val loss: 1.7194 | trn acc: 0.6462\n",
            "epoch: 1/1 | step: 6900/10000 | trn loss: 1.7438 | val loss: 1.7095 | trn acc: 0.6473\n",
            "epoch: 1/1 | step: 7000/10000 | trn loss: 1.7194 | val loss: 1.7038 | trn acc: 0.6487\n",
            "epoch: 1/1 | step: 7100/10000 | trn loss: 1.6915 | val loss: 1.7122 | trn acc: 0.6504\n",
            "epoch: 1/1 | step: 7200/10000 | trn loss: 1.7388 | val loss: 1.7114 | trn acc: 0.6515\n",
            "epoch: 1/1 | step: 7300/10000 | trn loss: 1.7103 | val loss: 1.7063 | trn acc: 0.6529\n",
            "epoch: 1/1 | step: 7400/10000 | trn loss: 1.7436 | val loss: 1.7108 | trn acc: 0.6537\n",
            "epoch: 1/1 | step: 7500/10000 | trn loss: 1.6928 | val loss: 1.7129 | trn acc: 0.6553\n",
            "epoch: 1/1 | step: 7600/10000 | trn loss: 1.6884 | val loss: 1.7059 | trn acc: 0.6569\n",
            "epoch: 1/1 | step: 7700/10000 | trn loss: 1.7093 | val loss: 1.7177 | trn acc: 0.6582\n",
            "epoch: 1/1 | step: 7800/10000 | trn loss: 1.7030 | val loss: 1.7151 | trn acc: 0.6594\n",
            "epoch: 1/1 | step: 7900/10000 | trn loss: 1.7219 | val loss: 1.7058 | trn acc: 0.6606\n",
            "epoch: 1/1 | step: 8000/10000 | trn loss: 1.6991 | val loss: 1.7142 | trn acc: 0.6618\n",
            "epoch: 1/1 | step: 8100/10000 | trn loss: 1.7387 | val loss: 1.7162 | trn acc: 0.6626\n",
            "epoch: 1/1 | step: 8200/10000 | trn loss: 1.7159 | val loss: 1.7088 | trn acc: 0.6637\n",
            "epoch: 1/1 | step: 8300/10000 | trn loss: 1.7118 | val loss: 1.7087 | trn acc: 0.6647\n",
            "epoch: 1/1 | step: 8400/10000 | trn loss: 1.7007 | val loss: 1.7062 | trn acc: 0.6658\n",
            "epoch: 1/1 | step: 8500/10000 | trn loss: 1.7097 | val loss: 1.7067 | trn acc: 0.6669\n",
            "epoch: 1/1 | step: 8600/10000 | trn loss: 1.7267 | val loss: 1.6955 | trn acc: 0.6678\n",
            "epoch: 1/1 | step: 8700/10000 | trn loss: 1.7069 | val loss: 1.7057 | trn acc: 0.6688\n",
            "epoch: 1/1 | step: 8800/10000 | trn loss: 1.7367 | val loss: 1.7056 | trn acc: 0.6695\n",
            "epoch: 1/1 | step: 8900/10000 | trn loss: 1.7152 | val loss: 1.7014 | trn acc: 0.6704\n",
            "epoch: 1/1 | step: 9000/10000 | trn loss: 1.7101 | val loss: 1.7000 | trn acc: 0.6712\n",
            "epoch: 1/1 | step: 9100/10000 | trn loss: 1.7070 | val loss: 1.7055 | trn acc: 0.6721\n",
            "epoch: 1/1 | step: 9200/10000 | trn loss: 1.7267 | val loss: 1.6974 | trn acc: 0.6729\n",
            "epoch: 1/1 | step: 9300/10000 | trn loss: 1.7249 | val loss: 1.7010 | trn acc: 0.6736\n",
            "epoch: 1/1 | step: 9400/10000 | trn loss: 1.7066 | val loss: 1.6987 | trn acc: 0.6745\n",
            "epoch: 1/1 | step: 9500/10000 | trn loss: 1.6836 | val loss: 1.7088 | trn acc: 0.6756\n",
            "epoch: 1/1 | step: 9600/10000 | trn loss: 1.6934 | val loss: 1.6996 | trn acc: 0.6766\n",
            "epoch: 1/1 | step: 9700/10000 | trn loss: 1.6850 | val loss: 1.7019 | trn acc: 0.6777\n",
            "epoch: 1/1 | step: 9800/10000 | trn loss: 1.6948 | val loss: 1.7058 | trn acc: 0.6787\n",
            "epoch: 1/1 | step: 9900/10000 | trn loss: 1.7051 | val loss: 1.7014 | trn acc: 0.6795\n",
            "epoch: 1/1 | step: 10000/10000 | trn loss: 1.7043 | val loss: 1.7144 | trn acc: 0.6803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "MbllLwpPSkW5",
        "outputId": "213d424f-92ce-4430-d084-5a1e5700dcab"
      },
      "source": [
        "plt.figure(figsize=(8,4.5))\n",
        "plt.plot(acc_list, label=\"CNN\", color='blue', linestyle='--')\n",
        "plt.plot(acc_list_r, label=\"RANDOM: Sigmoid\", color='red', linestyle='--')\n",
        "plt.legend()\n",
        "plt.xlim(0, 100)\n",
        "plt.xticks([0, 20, 40, 60, 80, 100],[0, 2000, 4000, 6000, 8000, 10000])\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel(\"Training iterations\")\n",
        "plt.ylabel(\"Test set accuracy (%)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Test set accuracy (%)')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x324 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxbAQXTHJVpM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}